{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f024bf9-b054-46f0-a5dd-a5dd84e13783",
   "metadata": {},
   "source": [
    "## Estensione del canale \n",
    "L'estensione $n$-esima di un **canale discreto senza memoria** è la tupla composta da:\n",
    "$$\n",
    "    <\\mathbb{X}^n,p(y^n\\mid x^n),\\mathbb{Y}^n,>\n",
    "$$\n",
    "Dato che manteniamo la proprietà di **assenza di memoria** il calcolo:\n",
    "$$\n",
    "    p(y^n\\mid x^n) = \\prod_{i=1}^{n} p(y_i\\mid x_i)\n",
    "$$\n",
    "*Semanticamente*: l'estensione $n$-esima del canale rappresenta quante volte sto utilizzando il canale per inviare informazioni, per $n = 5$ vuol dire che utilizzero il canale $5$-volte per inviare una sequenza di simboli lunga $5$.\n",
    "\n",
    "Ora definiamo un **codice canale** di tipo $(M,n)$ per il canale $<\\mathbb{X},\\mathbb{Y}, p(y\\mid x)>$ consiste in:\n",
    "- Un'insieme di indici $\\{1,\\dots,M\\}$ che identificano i messaggi che vogliamo spedire\n",
    "- Una funzione di codifica $x^n: \\{1,\\dots,M\\}\\rightarrow \\mathbb{X}^n$ (ad esempio se ho $M = 2,n = 3$ potrei avere la funzione che mappa $\\{000,111\\}$).\n",
    "- Una funzione di decodifica $g: y^n \\rightarrow \\{1,\\dots,M\\}$ (sostanzialmente mi arriva una sequenza specifica $y^n \\in \\mathbb{Y}$ e questa funzione me lo decodifica).\n",
    "\n",
    "Questo codice avrà una **probabilità di errore sulla funzione di decodifica**:\n",
    "$$\n",
    "    \\lambda_i = p(g(y^n) \\neq i \\mid X^n = x^n(i))\n",
    "$$\n",
    "Prendo il messaggio $i$-esimo ne faccio la codifica $x^n(i)$ e ricevo una $n$-upla dal canale $y^n$ e la decodifico con $g$: se la decodifica non mi indica il messaggio $i$ vuol dire che $g$ ha commesso un errore perchè la variabile aleatoria $X^n$ ha assunto la sequenza $x^n(i)$ del messaggio $i$.\n",
    "\n",
    "Possiamo inoltre definire altre **due probabilità di errore**\n",
    "- Probabilità **massima** d'errore: $\\lambda^{(n)} = \\max_{\\{i=1,\\dots,M\\}} \\lambda_i$( *Semanticamente*:qual è il messaggio che ha più alta probabilità di non essere decodificato correttamente?)\n",
    "- Probabilità **media** di errore: $P_e^{(n)} = \\frac{1}{M} \\sum_{i=1}^{M}\\lambda_i$(*Semanticamente*:mediamente qual è la probabilità di sbagliare utilizzando la funzione $g$).\n",
    "\n",
    "Da questo possiamo derivare il fatto che $P_e^{(n)}\\leq \\lambda^{(n)}$.\n",
    "\n",
    "Ora definiamo il **tasso di trasmissione** di un codice di tipo $(M,n)$:\n",
    "$$\n",
    "    R = \\frac{\\log_2 M}{n} \\leq 1 \\quad \\text{se $R = 1$ il canale è senza rumore}\n",
    "$$\n",
    "Definiamo inoltre che il tasso di trasmissione è *raggiungibile* se esiste una sequenza di codici di tipo $(\\lceil 2^{n\\cdot R}\\rceil, n)$ tale per cui la **probabilità di errore massima** $\\lambda^{(n)}$ per $n$ che tende a $\\infty$ è uguale a $0$:\n",
    "$$\n",
    "    \\lim_{n\\rightarrow \\infty} \\lambda^{(n)} = 0\n",
    "$$\n",
    "\n",
    "## Legge dei grandi numeri\n",
    "La **legge dei grandi numeri** ci dice che per ogni sequenza $X_1,\\dots,X_n$ di variabili aleatorie indipendenti e identicamente distribuite (i.i.d) con valore atteso finito cioè $\\mu < \\infty$ \n",
    "$$\n",
    "\\forall \\epsilon > 0  \\quad \\text{vale che} \\quad \\lim_{n \\rightarrow \\infty} p\\Big(\\mid\\frac{1}{n}\\sum_{i=1}^{n}x_i - \\mu\\mid > \\epsilon \\Big) = 0\n",
    "$$\n",
    "Sostanzialmente ci dice che all'aumentare di $n$ la media campionaria risulta pari al valore atteso $\\mu$.Cioè lo stimatore per $\\mu$ converge a $\\mu$.\n",
    "\n",
    "## Proprietà di Equipartizione Asintotica (AEP)\n",
    "Siano $X_1,\\dots,X_n$ variabili aleatorie *i.i.d* con valore atteso finito $\\mu < \\infty$, e che $\\forall \\epsilon > 0$ valga\n",
    "$$\n",
    "    \\lim_{n \\rightarrow \\infty} p\\Big(\\mid \\frac{1}{n} \\cdot \\log \\frac{1}{p(x_1) \\cdot p(x_2) \\cdot \\dots \\cdot p(x_n)} - H(X) \\mid > \\epsilon \\Big) = 0\n",
    "$$\n",
    "Questa proprietà è simile a quella della legge dei grandi numeri solo che quello che stiamo \"stimando\" è l'entropia $H(X)$.\n",
    "\n",
    "## Insieme Tipico\n",
    "Peschiamo $n$ oggetti creando una sequenza $(x_1,\\dots,x_n)$. Visto che le pescate sono *i.i.d*, la probabilità di pescare tale sequenza è il prodotto delle singole probabilità. Se questo prodotto è compreso tra due quantità che ora vedremo allora tale sequenza appartiene all’insieme tipico.\n",
    "$$\n",
    "    A_{\\epsilon}^{(n)} = \\{(x_1,\\dots,x_n)\\in \\mathbb{X}^n\\quad \\text{tali che}\\quad 2^{-n(H(X) + \\epsilon}\\leq \\prod_{i=1}^{n} p(x_i) \\leq 2^{-n(H(X) - \\epsilon)}\\}\n",
    "$$\n",
    "**l’insieme tipico è**, in realtà, l’insieme **formato da tutte le sequenze che rispettano\n",
    "la proprietà di equipartizione asintotica**. L'insieme tipico gode di queste due proprietà:\n",
    "- $\\lim_{n\\rightarrow \\infty} p(A_{\\epsilon}^{(n)}) = 1$\n",
    "- $\\lim_{n\\rightarrow \\infty} p((A_{\\epsilon}^{(n)})^{c}) = 0$ (il complementare è uguale a $0$)\n",
    "\n",
    "## Teorema \n",
    "Siano $X_1,X_2,\\dots,X_n$ variabili aleatorie *i.i.d* e sia $A_{\\epsilon}^{(n)}$ l'insieme tipico:\n",
    "1. $\\forall n$, $\\mid A_{\\epsilon}^{(n)} \\mid \\leq 2^{n\\cdot(H(X) + \\epsilon)}$\n",
    "2. $\\forall n$ sufficentemente grande $\\mid A_{\\epsilon}^{(n)} \\mid \\geq (1 - \\epsilon) \\cdot 2^{n\\cdot(H(X)-\\epsilon)}$\n",
    "Il nostro obbiettivo è quello di riempire di insiemi tipici, che non si sovrappongano, il codomino della funzione.\n",
    "\n",
    "Sapendo che $2^{n\\cdot H(Y|X)}$ è il numero di elementi possibili da un insieme "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff323b8-3913-4766-840b-8bbc91086492",
   "metadata": {},
   "source": [
    "## Secondo Teorema di Shannon\n",
    "\n",
    "Sia $<\\mathbb{X},\\mathbb{Y}, p(y\\mid x)>$ un modello di canale con capacità $C$. Per ogni tasso di trasmissione $R$ strettamente $R < C$, esiste una sequenza di codici $k_1,k_2,\\dots,k_n$ dove il codice $k_n$ è di tipo $(\\lceil 2^{n\\cdot R_n} \\rceil, n)$ ed è tale che:\n",
    "$$\n",
    "\\lim_{n\\rightarrow \\infty} R_n = R \\quad \\text{e}\\quad \\lim_{n\\rightarrow \\infty} \\lambda^{(n)}(k_n) = 0\n",
    "$$\n",
    "Sostanzialmente stiamo dicendo che dato un canale possiamo spedire informazione sempre inferiore alla sua capacità, e per farlo mi serve un codice tra i $k_1,k_2,\\dots,k_n$ che ha la forma del tipo $(\\lceil 2^{n\\cdot R_n} \\rceil, n)$. Dobbiamo sfoltire il numero di messaggi che risuciamo a inviare di un **tasso** $R_n$ questo perchè il canale è rumoroso. Se noi consideriamo il codice che ci permette di avvicinarci alla capacità del canale, perchè esiste sempre un codice che si spinge alla **capacità massima del canale**, tenendo sotto controllo l'errore massimo che potrei generare. Il secondo teorema di Shannon ci dice che questo codice esiste ma non ci da informazioni per trovarlo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
