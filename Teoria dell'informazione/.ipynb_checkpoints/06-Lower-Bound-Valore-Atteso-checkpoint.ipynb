{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "274dc206-5680-46a8-8814-21b21c813726",
   "metadata": {},
   "source": [
    "#### Nozioni preliminari\n",
    "1. **Entropia relativa**\n",
    "    $$\n",
    "        D(X || Y) = \\sum_{s \\in \\mathbb{S}} p_x(s) \\cdot \\log_D \\frac{p_x(s)}{p_y(s)}\n",
    "    $$\n",
    "2. **Disuguaglianza di Kraft**\n",
    "    $$\n",
    "       \\sum_{i=1}^{n} D^{-li} \\leq 1\n",
    "    $$\n",
    "___\n",
    "\n",
    "Vediamo un'importante risultato che lega l'**entropia** con il **valore atteso delle lunghezze** di un codice istantaneo.\n",
    "\n",
    "## Teorema\n",
    "Sia $\\mathbb{c}:\\mathbb{X} \\rightarrow D^+$, un codice istantaneo e $D$-ario per la sorgente $<\\mathbb{X},p>$ allora\n",
    "$$\n",
    "    E[l_c] \\geq H_D(X)\n",
    "$$\n",
    "### <mark>Dimostrazione</mark>\n",
    "Definisco una variabile aleatoria $Z:\\mathbb{X}\\rightarrow \\mathbb{R}$ con distribuzione di probabilità pari a \n",
    "$$\n",
    "q(x) = \\frac{D^{-l_c(x)}}{\\sum_{x' \\in \\mathbb{X}} D^{-l_c(x')}}\n",
    "$$\n",
    "L'obbiettivo della dimostrazione è arrivare a $E[l_c] - H_D(X) \\geq 0$. Sapendo che $E[l_c] = \\sum_{x \\in \\mathbb{X}} p(x) \\cdot l_c(x)$ riscriviamo questa disequazione come:\n",
    "\\begin{align}\n",
    "    &\\sum_{x \\in \\mathbb{X}} p(x) \\cdot l_c(x) - \\sum_{x \\in \\mathbb{X}} p(x) \\cdot \\log_D \\frac{1}{p(x)}\\geq 0 \\quad \\quad &(1)\n",
    "\\end{align}\n",
    "Riscriviamo ora $l_c$ come $l_c = \\log_D D^{l_c(x)}$\n",
    "\\begin{align}\n",
    "    &\\sum_{x \\in \\mathbb{X}} p(x) \\cdot \\log_D D^{l_c(x)} - \\sum_{x \\in \\mathbb{X}} p(x) \\cdot \\log_D \\frac{1}{p(x)}\\geq 0 \\quad \\quad &(2)\\\\\n",
    "    &\\sum_{x \\in \\mathbb{X}} p(x) \\cdot \\big[\\log_D D^{l_c(x)} - \\log_D \\frac{1}{p(x)} \\big]\\geq 0 \\quad \\quad &(3)\\\\\n",
    "    &\\sum_{x \\in \\mathbb{X}} p(x) \\cdot \\big[\\log_D D^{l_c(x)} + \\log_D p(x) \\big]\\geq 0 \\quad \\quad &(4)\\\\\n",
    "    &\\sum_{x \\in \\mathbb{X}} p(x) \\cdot \\big[\\log_D \\frac{p(x)}{D^{-l_c(x)}}\\big]\\geq 0 \\quad \\quad &(5)\\\\\n",
    "\\end{align}\n",
    "Il passaggio $(5)$ è possibile perchè \n",
    "$$\n",
    "\\big[\\log_D \\frac{p(x)}{D^{-l_c(x)}}\\big] = \\log_D p(x) - log_D D^{-l_c(x)} = \\log_D p(x) + log_D D^{l_c(x)}\n",
    "$$\n",
    "\n",
    "Moltiplico ora per $1 = \\frac{\\sum_{x'} D^{-l_c(x')}}{\\sum_{x'}{D^{-l_c(x')}}}$ e ottengo:\n",
    "\\begin{align}\n",
    "    &\\sum_{x \\in \\mathbb{X}} p(x) \\cdot \\big[\\log_D \\frac{p(x)}{D^{-l_c(x)}} \\cdot \\frac{\\sum_{x'} D^{-l_c(x')}}{\\sum_{x'}{D^{-l_c(x')}}}\\big]\\geq 0 \\quad \\quad &(6)\\\\\n",
    "    &\\sum_{x \\in \\mathbb{X}} p(x) \\cdot \\big[\\log_D p(x) \\cdot \\frac{\\sum_{x'} D^{-l_c(x')}}{D^{-l_c(x)}} + \\log_D \\frac{1}{\\sum_{x'} D^{-l_c(x')}}\\big]\\geq 0 \\quad \\quad &(7)\\\\\n",
    "    &\\sum_{x \\in \\mathbb{X}} p(x) \\cdot \\big[\\log_D p(x) \\cdot \\frac{\\sum_{x'} D^{-l_c(x')}}{D^{-l_c(x)}} - \\log_D \\sum_{x'} D^{-l_c(x')}\\big]\\geq 0 \\quad \\quad &(8)\\\\\n",
    "\\end{align}\n",
    "Negli ultimi passaggi $(7),(8)$ ho riscritto la differenza tra i due logaritmo in questa forma $\\log_D A \\cdot \\frac{1}{B}$ dove ho utilizzato come <mark>$A = \\frac{p(x)}{D^{-l_c(x)}} \\cdot \\sum_{x'} D^{-l_c(x')}$</mark> mentre <mark>$B = \\frac{1}{\\sum_{x'} D^{-l_c(x')}}$</mark>.\n",
    "\n",
    "$q(x) = \\frac{D^{-l_c(x)}}{\\sum_{x' \\in \\mathbb{X}} D^{-l_c(x')}}$\n",
    "Guardiamo ora solo la parte $A$ e notiamo che:\n",
    "\\begin{align}\n",
    "    &\\sum_{x \\in \\mathbb{X}} p(x) \\cdot \\log_D p(x) \\cdot \\frac{\\sum_{x'} D^{-l_c(x')}}{D^{-l_c(x)}} \\quad &(9)\\\\\n",
    "    &\\sum_{x \\in \\mathbb{X}} p(x) \\cdot \\log_D p(x) \\cdot \\frac{1}{q(x)} \\quad &(10)\n",
    "\\end{align}\n",
    "<mark>$\\frac{\\sum_{x'} D^{-l_c(x')}}{D^{-l_c(x)}}$</mark> è pari all'inverso della distribuzione di probabilità che abbiamo definito all'inizio $q(x)$.\n",
    "Ma quindi stiamo scrivendo **l'entropia relativa** tra le variabili $X$ e $Z$ che per definizione è $D(X|| Z) \\geq 0$.\n",
    "\\begin{align}\n",
    "    &D(X || Z) - \\sum_{x} p(x) - \\log_D \\sum_{x'} D^{-l_c(x')} \\geq 0 \\quad &(11)\\\\\n",
    "    &D(X || Z) - \\log_D \\big(\\sum_{x'} D^{-l_c(x')}\\big) \\cdot \\sum_{x} p(x) \\geq 0 \\quad &(12) \\\\\n",
    "    &D(X || Z) - \\log_D \\big(\\sum_{x'} D^{-l_c(x')}\\big) \\cdot 1 \\geq 0 \\quad &(13) \\\\\n",
    "\\end{align}\n",
    "Per la **disuguaglianza di Kraft** <mark>$\\sum_{x'} D^{-l_c(x')} \\leq 1$</mark> questo perchè abbiamo definito $\\mathbb{c}$ istantaneo. Data questa proprietà **sappiamo che il logaritmo assumera valori negativi** per sua definizione:\n",
    "\n",
    "<img src = \"Media/Logaritmo.png\" width= \"500px\" height =\"500px\"></img>\n",
    "\n",
    "Ma dato che abbiamo il segno meno davanti al logaritmo **nell'equazione** i valori assunti saranno positivi **ma quindi dato che stiamo sommando due quantità $\\geq 0$ allora la disequazione è vera**:\n",
    "\\begin{align}\n",
    "  &D(X || Z) - \\log_D \\big(\\sum_{x'} D^{-l_c(x')}\\big) \\geq 0 \\quad &(13) \\\\\n",
    "  &D(X || Z) \\geq 0 \\quad &(14)\\\\\n",
    "  &- \\log_D \\big(\\sum_{x'} D^{-l_c(x')}\\big) \\geq 0 \\quad &(14)\\\\\n",
    "\\end{align}\n",
    "Allora abbiamo dimostrato che <mark>$E[l_c] - H_D(X) \\geq 0$</mark>. \n",
    "<div style=\"background-color: #D1D7FF; padding: 10px; border-radius: 5px;text-align: center; font-size:16px;\">\n",
    "<b>Semanticamente</b> abbiamo trovato che il valore atteso delle lunghezze deve essere come <b>minimo</b> maggiore o uguale dell'<b>entropia</b>.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
