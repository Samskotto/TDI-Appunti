{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c927360d-ccc5-49c5-ac5b-7bf1e6b84005",
   "metadata": {},
   "source": [
    "## Nozioni matematiche preliminari\n",
    "1. Il **cambio di base tra logaritmi**\n",
    "$$\n",
    "    \\log_b p = \\frac{log_a p}{log_a b}\n",
    "$$\n",
    "2. **Cambio di base sull'entropia**\n",
    "\\begin{align}\n",
    "   H_b(X) &= \\log_b a \\cdot \\sum_{i=1}^{m} p_i \\cdot log_a \\frac{1}{p_i}\\\\\n",
    "   &= \\log_b a \\cdot H_a(X)\n",
    "\\end{align}\n",
    "4. **Upper/Lower bound** della funzione $\\ln x$\n",
    "    <img src=\"Media/LowerUpperBound.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b2b9c3-6885-4990-9ae8-fbec1fba2f64",
   "metadata": {},
   "source": [
    "## Entropia e valore atteso delle lunghezze\n",
    "\n",
    "Sia la sorgente $<\\mathbb{X}, p>$ che emette $\\mathbb{X} = \\{x_1,\\dots, x_m\\}$ simboli che hanno una certa distribuzione di probabilità $p = \\{p_1,\\dots,p_m\\}$. Sia $X$ una variabile aleatoria discreta definita come:\n",
    "$$\n",
    "    X:\\mathbb{X} \\rightarrow \\{a_1,\\dots,a_m\\} \\quad \\text{con } a_1,\\dots,a_m \\in \\mathbb{R}\n",
    "$$\n",
    "Tale che $p(X = a_i) = p_i$ ($X$ rappresenta ciò che fa la sorgente estraendo un simbolo con probabilità $p_i$). \n",
    "\n",
    "Definiamo l'**entropia** della variabile $X$ come la misura della sua **incertezza**.\n",
    "$$\n",
    "    H(X) = \\sum_{i=1}^{m} p_i \\log\\frac{1}{p_i}\n",
    "$$\n",
    "*Note*: \n",
    "- L'entropia come l'abbiamo definita per comodità ha come unità di misura i bits e la base scelta è $D = 2$.\n",
    "- L'entropia $H(X) \\geq 0$.\n",
    "\n",
    "Ad esempio presa $X$ una variabile aleatoria **bernoulliana**, cioè composta da soli due esiti possibili $0$ e $1$, l'entropia si presenta in questa forma\n",
    "\\begin{cases}\n",
    "    p(X = 1)&= p\\\\\n",
    "    p(X = 0)&= 1-p \n",
    "\\end{cases}\n",
    "<img src=\"Media/EntropiaBernoulliana.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6394bb2-2f10-40f7-ab08-b083c24e74ca",
   "metadata": {},
   "source": [
    "L'entropia misura quanto gli eventi sono casuali e raggiungiamo il massimo della casualità quando gli eventi sono equiprobabili. \n",
    "**Ma fino a quali valori può assumere**, qual è quindi il suo **Upper Bound**?\n",
    "\n",
    "### Definizione\n",
    "Sia $X$ una variabile aleatoria che assume $a_1, \\dots, a_m$ valori distinti, allora $H_D(X)$ sarà certamente\n",
    "$$\n",
    "    H_D(X) \\leq log_D m \\quad \\forall D > 1\n",
    "$$\n",
    "Inoltre <mark>la funzione $H_D(X) = \\log_D m$ se e solo se $X$ ha una distribuzione di probabilità uniforme(quindi **equiprobabile**)</mark> sui simboli $a_1,\\dots,a_m$\n",
    "\n",
    "### <mark>Dimostrazione<mark>\n",
    "Vogliamo dimostrare che $H_D(X) - \\log_D m \\leq 0$\n",
    "Riscriviamo\n",
    "\\begin{align}\n",
    "    \\sum_{i=1}^{m} p_i \\cdot \\log_D \\frac{1}{p_i} - \\log_Dm \\quad \\quad (1)\n",
    "\\end{align}\n",
    "Ora moltiplico il logaritmo per $1$ e sostituisco l'$1$ con $\\sum_{i=1}^{m} p_i = 1$\n",
    "\\begin{align}\n",
    "    \\sum_{i=1}^{m} p_i \\cdot \\log_D \\frac{1}{p_i} - \\log_D m \\cdot \\sum_{i=1}^{m} p_i \\quad \\quad (2)\n",
    "\\end{align}\n",
    "Porto dentro la costante e raccolgo la sommatoria\n",
    "\\begin{align}\n",
    "    \\sum_{i=1}^{m} p_i [\\log_D \\frac{1}{p_i} - \\log_D m] \\rightarrow \\sum_{i=1}^{m} p_i [\\log_D \\frac{1}{p_i \\cdot m}] \\quad \\quad (3)\n",
    "\\end{align}\n",
    "Effettuiamo un cambio di base(da base $D$ a base $e$ naturale)\n",
    "\\begin{align}\n",
    "   \\sum_{i=1}^{m} p_i [\\frac{\\ln \\frac{1}{p_i \\cdot m}}{\\ln D}] \\quad \\quad (4)\n",
    "\\end{align}\n",
    "Porto la costante $\\frac{1}{\\ln D}$ fuori dalla sommatoria\n",
    "\\begin{align}\n",
    " \\frac{1}{\\ln D} \\cdot \\sum_{i=1}^{m} p_i [\\ln \\frac{1}{p_i \\cdot m}] \\quad \\quad (5)\n",
    "\\end{align}\n",
    "Applico la maggiorazione cioè l'upperbound del $\\ln x$ cioè $x-1$ dove $x = \\frac{1}{p_i \\cdot m}$\n",
    "\\begin{align}\n",
    " \\frac{1}{\\ln D} \\cdot \\sum_{i=1}^{m} p_i [\\ln \\frac{1}{p_i \\cdot m}] \\leq \\frac{1}{\\ln D} \\cdot \\sum_{i=1}^{m} p_i [\\frac{1}{p_i \\cdot m} - 1] \\quad \\quad (6)\n",
    "\\end{align}\n",
    "Ora guardo **solo il lato destro della diseguaglianza** e distribuisco $p_i$\n",
    "$$\n",
    "\\frac{1}{\\ln D} \\cdot [\\sum_{i=1}^{m}\\frac{p_i}{p_i \\cdot m} - p_i] \\quad \\quad (7)\n",
    "$$\n",
    "Separo le due sommatorie e ottengo\n",
    "$$\n",
    "\\frac{1}{\\ln D} \\cdot [\\sum_{i=1}^{m}\\frac{p_i}{p_i \\cdot m} - \\sum_{i=1}^{m} p_i] \\quad \\quad (8)\n",
    "$$\n",
    "Risolvo sapendo che $\\sum_{i=1}^{m} p_i = 1$\n",
    "$$\n",
    "\\frac{1}{\\ln D} \\cdot [\\sum_{i=1}^{m}\\frac{1}{m} - 1] \\quad \\quad (9)\n",
    "$$\n",
    "La $\\sum_{i=1}^{m}\\frac{1}{m}$ non ha nessun termine dipendente da $i$ quindi posso riscriverla come $m \\cdot \\frac{1}{m}$ \n",
    "$$\n",
    "\\frac{1}{\\ln D}[m \\cdot \\frac{1}{m} - 1] \\rightarrow \\frac{1}{\\ln D}\\cdot [0] \\rightarrow H_D - \\log_D m \\leq 0 \\quad \\quad (10)\n",
    "$$\n",
    "Abbiamo dimostrato che <mark>$H_D - \\log_D m \\leq 0 \\rightarrow H_D \\leq  \\log_D m$</mark>\n",
    "Quando tutti gli oggetti possono essere estratti con la stessa probabilità abbiamo che\n",
    "$$\n",
    "    H_D(X) = \\sum_{i=1}^{m} \\frac{1}{m} \\cdot \\log_Dm = \\log_Dm \\cdot \\sum_{i=1}^{m} \\frac{1}{m} = \\log_Dm \\quad \\text{se } p_i = \\frac{1}{m} \\forall i \n",
    "$$\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
