{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f476ee-b696-4c1a-bda6-d96de7202705",
   "metadata": {},
   "source": [
    "Abbiamo visto i limiti **superiori e inferiori** che deve avere $E[l_c]$ utilizzando il **codice di Shannon**, formalmente\n",
    "$$\n",
    "    H_D(X) \\leq E[l_c] \\leq H_D(X) + 1 \n",
    "$$\n",
    "<mark>Shannon capisce che il suo codice è inefficente perchè quel **bit** di errore mediamente cresce lineramente all'aumentare della lunghezza del messaggio da codificare</mark>.\n",
    "___\n",
    "## Entropia e Codifica a blocchi\n",
    "Per questo motivo passiamo alla **codifica a blocchi**. Se consideriamo blocchi di dimensione fissata $n$ la sorgente risulta $<X^n,P_n>$ dove $P_n = \\prod_{i = 1}^{n} p(x_i)$ (cioè la produttoria delle **probabilità dei singoli simboli del blocco** di lunghezza $n$). Riscriviamo l'**entropia** come \n",
    "$$\n",
    "H(X_1,\\dots,X_n) = \\sum_{x_1,\\dots,x_n} P_n(x_1,\\dots,x_n) \\log \\frac{1}{P_n(x_1,\\dots,x_n)} \\quad (1)\n",
    "$$\n",
    "*Semanticamente* stiamo calcolando l'entropia di $n$-variabili aleatorie che pescano una lettera contemporaneamente in modo da formare il blocco di lunghezza $n$. Possiamo riscrivere meglio l'**entropia** come: \n",
    "\\begin{align}\n",
    "    &\\sum_{x_1}\\dots\\sum_{x_n} \\big(\\prod_{i=1}^{n} p(x_i) \\big) \\cdot \\log \\frac{1}{\\prod_{i=1}^{n} p(x_i)} \\quad &(2)\\\\\n",
    "    &\\sum_{x_1}\\dots\\sum_{x_n} \\big(\\prod_{i=1}^{n} p(x_i) \\big) \\cdot \\log \\big( \\prod_{i=1}^{n} p(x_i)\\big)^{-1} \\quad &(3)\\\\\n",
    "    &\\sum_{x_1}\\dots\\sum_{x_n} \\big(\\prod_{i=1}^{n} p(x_i) \\big) \\cdot \\sum_{i=1}^{n}\\log \\big( \\frac{1}{p(x_i)}\\big) \\quad &(4)\\\\\n",
    "\\end{align}\n",
    "\n",
    "Nei punti $(3),(4)$ ho utilizzato una delle proprietà dei logaritmi cioè <mark>la **somma di logaritmi con la stessa base** è uguale al logaritmo del prodotto dei loro argomenti</mark>.\n",
    "\\begin{align}\n",
    "    &\\sum_{i=1}^{n} \\sum_{x_1}\\dots\\sum_{x_n} \\big(\\prod_{i=1}^{n} p(x_i) \\big) \\cdot \\log \\big( \\frac{1}{p(x_i)}\\big) \\quad &(5)\\\\\n",
    "\\end{align}\n",
    "Facciamo un esempio per $n = 2$:\n",
    "\\begin{align}\n",
    "    &\\sum_{x_1}\\sum_{x_2} \\big(\\prod_{i=1}^{2} p(x_i) \\big) \\cdot \\big( \\log \\frac{1}{p(x_1)} + \\log \\frac{1}{p(x_2)}\\big) \\quad &(6)\\\\\n",
    "    &\\sum_{x_1}\\sum_{x_2} \\big(p(x_1)\\cdot p(x_2) \\big) \\cdot \\big( \\log \\frac{1}{p(x_1)} + \\log \\frac{1}{p(x_2)}\\big) \\quad &(7)\\\\\n",
    "    &\\sum_{x_1}\\sum_{x_2} \\big(p(x_1)\\cdot p(x_2) \\big) \\cdot \\log \\frac{1}{p(x_1)} + \\big(p(x_1)\\cdot p(x_2) \\big) \\cdot \\log \\frac{1}{p(x_2)}\\big) \\quad &(8)\\\\\n",
    "    &\\sum_{x_1}\\sum_{x_2} \\big(p(x_1)\\cdot p(x_2) \\big) \\cdot \\log \\frac{1}{p(x_1)} + \\sum_{x_1}\\sum_{x_2} \\big(p(x_1)\\cdot p(x_2) \\big) \\cdot \\log \\frac{1}{p(x_2)}\\big) \\quad &(9)\\\\\n",
    "    &\\sum_{x_2} p(x_2) \\cdot \\sum_{x_1} \\big(p(x_1)\\big) \\cdot \\log \\frac{1}{p(x_1)} + \\sum_{x_1} p(x_1)\\cdot \\sum_{x_2} \\big(p(x_2)\\big) \\cdot \\log \\frac{1}{p(x_2)}\\big) \\quad &(10)\\\\\n",
    "    &1 \\cdot \\sum_{x_1} \\big(p(x_1)\\big) \\cdot \\log \\frac{1}{p(x_1)} + 1 \\cdot \\sum_{x_2} \\big(p(x_2)\\big) \\cdot \\log \\frac{1}{p(x_2)}\\big) \\quad &(11)\\\\\n",
    "    &H(X_1) + H(X_2) \\quad &(12)\\\\\n",
    "\\end{align}\n",
    "Nei punti $(10),(11)$ abbiamo esplicitato la sommatoria sulle porbabilità (che da come risultato $1$) facendo vedere quindi che l'entropia congiunta tra due variabili aleatorie è pari alla loro somma. Quindi $H(X_1,\\dots,X_n) = n \\cdot H(X)$ cioè l'entropia calcolata sulla sorgente originale moltiplicata per la dimensione del blocco.\n",
    "___\n",
    "## Teorema\n",
    "Sia $C_n:\\mathbb{X^n} \\rightarrow D^+$ un codice di shannon $D$-ario a blocchi per la sorgente $<\\mathbb{X}, p>$ dove $l_{C_n}(x_1,\\dots,x_n) = \\lceil \\log_D \\frac{1}{P_n(x_1,\\dots,x_n)}\\rceil$ allora\n",
    "$$\n",
    "    \\lim_{n \\rightarrow \\infty} \\frac{1}{n} E[l_{C_n}] = H_D(X)\n",
    "$$\n",
    "Dove $X$ è la variabile casuale che estrae sulla sorgente di partenza $<\\mathbb{X},p>$\n",
    "### Dimostrazione\n",
    "Riprendendo la disuguaglianza con **l'upper/lower bound** del valore atteso delle lunghezze del codice, possiamo riscrivere:\n",
    "\\begin{align}\n",
    "    &H_D(X_1,\\dots,X_n) \\leq E[l_{C_n}] < H_D(X_1,\\dots,X_n) + 1 \\quad &(1)\\\\\n",
    "    \\\\\n",
    "    &H_D(X)\\cdot n \\leq E[l_{C_n}] < H_D(X)\\cdot n + 1 \\quad &(2)\n",
    "\\end{align}\n",
    "Dividendo per $n$ otteniamo questa disuguaglianza\n",
    "\\begin{align}\n",
    "    &H_D(X) \\leq \\frac{1}{n}E[l_{C_n}] < H_D(X) + \\frac{1}{n} \\quad &(3)\\\\\n",
    "\\end{align}\n",
    "Da qui vediamo che se prendiamo il limite e facciamo tendere $n$ all'infinito il valore atteso si schiaccia sull'**entropia**.\n",
    "\n",
    "<div style=\"background-color: #D1D7FF; padding: 10px; border-radius: 5px;text-align: center; font-size:16px;\">\n",
    "<i>Semanticamente</i>: il primo teorema di Shannon ci dice che se tendiamo a $\\infty$ la grandezza del blocco, allora il <b>codice è ottimale</b>, perché l'upper bound coincide con il lower bound che avevamo definito su $E[l_c]$, di conseguenza abbiamo <b>compresso il messaggio con il nostro codice</b> al massimo possibile.\n",
    "</div>\n",
    "\n",
    "**Perchè nella realtà questo teorema non è praticabile?**\n",
    "La complessità del **codice a blocchi** aumenta esponenzialmente in base alla lunghezza $n$ del blocco. \n",
    "\n",
    "Ad esempio: se ho una sorgente $\\mathbb{X}$ con cardinalità $|\\mathbb{X}| = 100$ e un codice a blocchi $C_n$ con $n = 10$ dovrò codificare $100^{10}$ parole del mio codice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5424071-5a3d-4132-b570-a43c13205a76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
